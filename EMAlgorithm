% ====== Unit Gamma Mixture Regression EM Algorithm ======
clc; clear; close all;

%% ====== Step 0: Data Simulation via Inverse CDF ======
n = 500; p = 2; K = 2;
beta_true  = [1 -1; -1 2];   % Each column is beta for group k
alpha_true = [2, 4];        % True alpha for each group
pi_true    = [0.6, 0.4];     % Mixing weights

x_vals = normrnd(0,1,n,1);
X      = [ones(n,1), x_vals];     % Design matrix

z = randsample(1:K, n, true, pi_true);
y = zeros(n,1);
for i=1:n
    k0 = z(i);
    eta = X(i,:)*beta_true(:,k0);
    mu  = 1/(1+exp(-eta));
    mu_alpha = mu^(1/alpha_true(k0));
    lambda   = mu_alpha/(1-mu_alpha);
    t = gammaincinv(rand(), alpha_true(k0), 'lower');
    y(i) = exp(-t/lambda);
end
y = max(y, eps);

%% ====== EM Initialization (Step 1) ======
reg     = 1e-6;                   % Regularization for numerical stability
k       = 1;                     % EM counter
max_iter= 200; tol = 1e-6;       % Max iterations and tolerance
tau     = rand(n,K);
tau     = tau./sum(tau,2);       % Random responsibilities
pi_k    = ones(1,K)/K;           % Initial mixing weights
beta_k  = randn(p,K);            % Initial regression coefficients
alpha_k = 1.5*ones(1,K);         % Initial alpha values
converged = false;

while k <= max_iter && ~converged
    %% ====== Step 2: E-step ======
    dens = zeros(n,K);
    for j = 1:K
        eta = X * beta_k(:,j);
        mu  = 1./(1+exp(-eta));
        mu_alpha = mu.^(1/alpha_k(j));
        lambda   = mu_alpha./(1-mu_alpha);
        logpdf = alpha_k(j)*log(lambda) - gammaln(alpha_k(j)) + ...
                 (lambda-1).*log(y) + (alpha_k(j)-1).*log(-log(y));
        dens(:,j) = pi_k(j) * exp(logpdf);
    end
    tau = dens ./ sum(dens,2);    % Posterior zij(k)

    %% ====== Step 3: M-step (pi update) ======
    Nk = sum(tau,1);
    pi_new = Nk / n;

    %% ====== Step 4: M-step (beta update) ======
    beta_new = beta_k;
    for j = 1:K
        b_curr = beta_k(:,j);
        a_curr = alpha_k(j);
        for it = 1:5
            eta = X * b_curr;
            u   = 1./(1+exp(-eta));
            u   = max(min(u, 1-eps), eps);
            
            u_alpha = u.^(1/a_curr);
            one_mu  = 1 - u_alpha;
            one_mu  = max(one_mu, eps);
            
            lambda  = u_alpha./(one_mu + eps);
            lny = log(y + eps);
            
            term1 = (1-u)./(one_mu + eps);
            term2 = (u_alpha.*(1-u).*lny)./(a_curr*(one_mu + eps).^2);
            score = sum(tau(:,j).*(term1+term2).*X,1)';
            
            fisher = sum(tau(:,j).*((1-u).^2)./(a_curr*(one_mu + eps).^2).*(X.^2),1);
            fisher = max(fisher, eps);
            
            J = diag(fisher) + reg*eye(p);
            
            if rcond(J) < eps
                warning('Fisher 矩陣接近奇異，增加正則化');
                J = J + reg*eye(p);
            end
            
            try
                b_curr = b_curr + (J \ score);
            catch
                warning('使用備用方法求解');
                b_curr = b_curr + pinv(J) * score;
            end
            
            max_update = 1.0;
            update = b_curr - beta_k(:,j);
            if norm(update) > max_update
                update = update * (max_update / norm(update));
                b_curr = beta_k(:,j) + update;
            end
        end
        beta_new(:,j) = b_curr;
    end

    %% ====== Step 5: M-step (alpha update) ======
    alpha_new = alpha_k;
    for j = 1:K
        delta = log(alpha_k(j));
        for it = 1:5
            alpha = exp(delta);
            eta = X * beta_new(:,j);
            u   = 1./(1+exp(-eta));
            u_alpha = u.^(1/alpha);
            one_mu = 1 - u_alpha;
            lny = log(y);
            logu = log(u);
            loglogy = log(-log(y));

            d_ll_dalpha = -logu./(alpha^2 .* one_mu);
            d_lambda = (u_alpha./one_mu) .* d_ll_dalpha;
            term1 = log(u_alpha./one_mu);
            term2 = d_ll_dalpha .* alpha;
            term3 = d_lambda .* lny;
            dlogpdf = term1 + term2 + term3 - psi(alpha) + loglogy;
            score_a = alpha * sum(tau(:,j) .* dlogpdf);

            trig = psi(1,alpha);
            t1 = -(logu.*(2*alpha - logu - 2*alpha.*u_alpha)) ./ (alpha .* one_mu.^2);
            t2 = alpha^2*trig;
            t3 = alpha*psi(alpha);
            t4 = -alpha*log(log(1./y));
            t5 = -alpha*log(u_alpha./one_mu);
            fisher_a = sum(tau(:,j).*(t1+t2+t3+t4+t5));

            fisher_a = fisher_a + reg;          % Avoid division by zero
            delta = delta + score_a / fisher_a;
        end
        alpha_new(j) = exp(delta);
    end

    %% ====== Step 6: Check convergence ======
    diff_val = sum((pi_new - pi_k).^2) + sum((beta_new(:) - beta_k(:)).^2) + sum((alpha_new - alpha_k).^2);
    if diff_val < tol
        fprintf('Converged at iteration %d (EM step), diff=%.3e\n', k, diff_val);
        converged = true;
    end

    pi_k    = pi_new;
    beta_k  = beta_new;
    alpha_k = alpha_new;
    k = k + 1;
end

%% ====== Results ======
disp('Estimated mixing weights pi_k:'); disp(pi_k);
disp('Estimated regression coefs beta_k:'); disp(beta_k);
disp('Estimated alphas alpha_k:'); disp(alpha_k);

%% ====== Visualization: Combined True Groups + Estimated Regression Lines ======
figure; hold on;
colors = [0.30 0.75 0.93; 0.94 0.45 0.40; 0.50 0.80 0.50]; % 柔和對比色，最多3群
markersize = 18;

% 畫出真實分群散點圖（依照 z 真實群組）
for j = 1:K
    idx = (z == j);
    scatter(X(idx,2), y(idx), markersize, 'MarkerFaceColor', colors(j,:), ...
            'MarkerEdgeColor', 'k', 'MarkerFaceAlpha', 0.6, ...
            'DisplayName', sprintf('Group %d (True)', j));
end

% 畫出每一群估計的 μ(x) 回歸曲線（實線，較粗）
x_grid = linspace(min(X(:,2)), max(X(:,2)), 300)';
X_grid = [ones(size(x_grid)), x_grid];
for j = 1:K
    mu_pred = 1 ./ (1 + exp(-X_grid * beta_k(:,j)));
    plot(x_grid, mu_pred, '-', 'LineWidth', 3, 'Color', colors(j,:), ...
         'DisplayName', sprintf('Group %d μ(x)', j));
end

xlabel('x', 'FontSize', 12); 
ylabel('y / \mu(x)', 'FontSize', 12);
title('True Groups with Estimated Regression Lines', 'FontSize', 14);
legend('Location','best');
grid on;
box on;
